{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb588be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Using cached langchain-1.0.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting langchain-pinecone\n",
      "  Using cached langchain_pinecone-0.2.12-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting langchain-huggingface\n",
      "  Using cached langchain_huggingface-1.0.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: pinecone-client in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (6.0.0)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (5.1.1)\n",
      "Requirement already satisfied: dotenv in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (0.9.9)\n",
      "Collecting langchain-core<2.0.0,>=1.0.0 (from langchain)\n",
      "  Using cached langchain_core-1.0.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting langgraph<1.1.0,>=1.0.0 (from langchain)\n",
      "  Using cached langgraph-1.0.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Using cached pydantic-2.12.3-py3-none-any.whl.metadata (87 kB)\n",
      "INFO: pip is looking at multiple versions of langchain-pinecone to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain-pinecone\n",
      "  Using cached langchain_pinecone-0.2.11-py3-none-any.whl.metadata (6.1 kB)\n",
      "  Using cached langchain_pinecone-0.2.10-py3-none-any.whl.metadata (5.3 kB)\n",
      "  Using cached langchain_pinecone-0.2.9-py3-none-any.whl.metadata (5.3 kB)\n",
      "  Using cached langchain_pinecone-0.2.8-py3-none-any.whl.metadata (5.3 kB)\n",
      "  Using cached langchain_pinecone-0.2.7-py3-none-any.whl.metadata (5.3 kB)\n",
      "  Using cached langchain_pinecone-0.2.6-py3-none-any.whl.metadata (5.3 kB)\n",
      "  Using cached langchain_pinecone-0.2.5-py3-none-any.whl.metadata (1.3 kB)\n",
      "INFO: pip is still looking at multiple versions of langchain-pinecone to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached langchain_pinecone-0.2.4-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Using cached langchain_pinecone-0.2.3-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Using cached langchain_pinecone-0.0.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting numpy<2,>=1 (from langchain-pinecone)\n",
      "  Using cached numpy-1.26.4.tar.gz (15.8 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): still running...\n",
      "  Preparing metadata (pyproject.toml): still running...\n",
      "  Preparing metadata (pyproject.toml): still running...\n",
      "  Preparing metadata (pyproject.toml): still running...\n",
      "  Preparing metadata (pyproject.toml): still running...\n",
      "  Preparing metadata (pyproject.toml): still running...\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting langchain\n",
      "  Using cached langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.72 (from langchain)\n",
      "  Using cached langchain_core-0.3.79-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain)\n",
      "  Using cached langchain_text_splitters-0.3.11-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting langsmith>=0.1.17 (from langchain)\n",
      "  Using cached langsmith-0.4.37-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Using cached sqlalchemy-2.0.44-cp313-cp313-win_amd64.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (from langchain) (2.32.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (from langchain) (6.0.3)\n",
      "Requirement already satisfied: pinecone<8.0.0,>=6.0.0 in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (7.3.0)\n",
      "Requirement already satisfied: numpy>=1.26.4 in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (from langchain-pinecone) (2.3.4)\n",
      "Collecting langchain-openai>=0.3.11 (from langchain-pinecone)\n",
      "  Using cached langchain_openai-1.0.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: httpx>=0.28.0 in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (from langchain-pinecone) (0.28.1)\n",
      "Collecting simsimd>=5.9.11 (from langchain-pinecone)\n",
      "  Using cached simsimd-6.5.3-cp313-cp313-win_amd64.whl.metadata (71 kB)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<1.0.0,>=0.3.72->langchain)\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting jsonpatch<2.0.0,>=1.33.0 (from langchain-core<1.0.0,>=0.3.72->langchain)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.15.0)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (24.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.33.4 in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (from langchain-huggingface) (0.35.3)\n",
      "INFO: pip is looking at multiple versions of langchain-huggingface to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain-huggingface\n",
      "  Using cached langchain_huggingface-0.3.1-py3-none-any.whl.metadata (996 bytes)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (from langchain-huggingface) (0.22.1)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (from pinecone-client) (2025.10.5)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (from pinecone-client) (0.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (from pinecone-client) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (from pinecone-client) (2.5.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (from sentence-transformers) (4.57.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (from sentence-transformers) (2.9.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (from sentence-transformers) (1.16.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (from sentence-transformers) (12.0.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (from dotenv) (1.1.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (from httpx>=0.28.0->langchain-pinecone) (4.11.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (from httpx>=0.28.0->langchain-pinecone) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (from httpx>=0.28.0->langchain-pinecone) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.28.0->langchain-pinecone) (0.16.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2025.9.0)\n",
      "INFO: pip is looking at multiple versions of langchain-openai to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain-openai>=0.3.11 (from langchain-pinecone)\n",
      "  Using cached langchain_openai-0.3.35-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting openai<3.0.0,>=1.104.2 (from langchain-openai>=0.3.11->langchain-pinecone)\n",
      "  Using cached openai-2.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting tiktoken<1.0.0,>=0.7.0 (from langchain-openai>=0.3.11->langchain-pinecone)\n",
      "  Using cached tiktoken-0.12.0-cp313-cp313-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting orjson>=3.9.14 (from langsmith>=0.1.17->langchain)\n",
      "  Using cached orjson-3.11.3-cp313-cp313-win_amd64.whl.metadata (43 kB)\n",
      "Collecting requests-toolbelt>=1.0.0 (from langsmith>=0.1.17->langchain)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard>=0.23.0 (from langsmith>=0.1.17->langchain)\n",
      "  Using cached zstandard-0.25.0-cp313-cp313-win_amd64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: pinecone-plugin-assistant<2.0.0,>=1.6.0 in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (from pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (1.8.0)\n",
      "Collecting aiohttp>=3.9.0 (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone)\n",
      "  Using cached aiohttp-3.13.1-cp313-cp313-win_amd64.whl.metadata (8.4 kB)\n",
      "Collecting aiohttp-retry<3.0.0,>=2.9.1 (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone)\n",
      "  Using cached aiohttp_retry-2.9.1-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.4 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached pydantic_core-2.41.4-cp313-cp313-win_amd64.whl.metadata (7.4 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (from python-dateutil>=2.5.3->pinecone-client) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.4)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Using cached greenlet-3.2.4-cp313-cp313-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.9.18)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (25.4.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone)\n",
      "  Using cached frozenlist-1.8.0-cp313-cp313-win_amd64.whl.metadata (21 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone)\n",
      "  Using cached multidict-6.7.0-cp313-cp313-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone)\n",
      "  Using cached propcache-0.4.1-cp313-cp313-win_amd64.whl.metadata (14 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone)\n",
      "  Using cached yarl-1.22.0-cp313-cp313-win_amd64.whl.metadata (77 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai<3.0.0,>=1.104.2->langchain-openai>=0.3.11->langchain-pinecone)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.10.0 (from openai<3.0.0,>=1.104.2->langchain-openai>=0.3.11->langchain-pinecone)\n",
      "  Using cached jiter-0.11.1-cp313-cp313-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (from openai<3.0.0,>=1.104.2->langchain-openai>=0.3.11->langchain-pinecone) (1.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\arnav\\onedrive\\desktop\\ikarus\\notebooks\\venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Using cached langchain-0.3.27-py3-none-any.whl (1.0 MB)\n",
      "Using cached langchain_pinecone-0.2.12-py3-none-any.whl (25 kB)\n",
      "Using cached langchain_core-0.3.79-py3-none-any.whl (449 kB)\n",
      "Using cached simsimd-6.5.3-cp313-cp313-win_amd64.whl (94 kB)\n",
      "Using cached langchain_huggingface-0.3.1-py3-none-any.whl (27 kB)\n",
      "Using cached langchain_openai-0.3.35-py3-none-any.whl (75 kB)\n",
      "Using cached langchain_text_splitters-0.3.11-py3-none-any.whl (33 kB)\n",
      "Using cached langsmith-0.4.37-py3-none-any.whl (396 kB)\n",
      "Using cached pydantic-2.12.3-py3-none-any.whl (462 kB)\n",
      "Using cached pydantic_core-2.41.4-cp313-cp313-win_amd64.whl (2.0 MB)\n",
      "Using cached sqlalchemy-2.0.44-cp313-cp313-win_amd64.whl (2.1 MB)\n",
      "Using cached aiohttp-3.13.1-cp313-cp313-win_amd64.whl (450 kB)\n",
      "Using cached aiohttp_retry-2.9.1-py3-none-any.whl (10.0 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached greenlet-3.2.4-cp313-cp313-win_amd64.whl (299 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached openai-2.5.0-py3-none-any.whl (999 kB)\n",
      "Using cached orjson-3.11.3-cp313-cp313-win_amd64.whl (131 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Using cached tiktoken-0.12.0-cp313-cp313-win_amd64.whl (879 kB)\n",
      "Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Using cached zstandard-0.25.0-cp313-cp313-win_amd64.whl (506 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached frozenlist-1.8.0-cp313-cp313-win_amd64.whl (43 kB)\n",
      "Using cached jiter-0.11.1-cp313-cp313-win_amd64.whl (203 kB)\n",
      "Using cached multidict-6.7.0-cp313-cp313-win_amd64.whl (45 kB)\n",
      "Using cached propcache-0.4.1-cp313-cp313-win_amd64.whl (40 kB)\n",
      "Using cached yarl-1.22.0-cp313-cp313-win_amd64.whl (86 kB)\n",
      "Installing collected packages: simsimd, zstandard, typing-inspection, tenacity, pydantic-core, propcache, orjson, multidict, jsonpatch, jiter, greenlet, frozenlist, distro, annotated-types, aiohappyeyeballs, yarl, tiktoken, SQLAlchemy, requests-toolbelt, pydantic, aiosignal, openai, langsmith, aiohttp, langchain-core, aiohttp-retry, langchain-text-splitters, langchain-openai, langchain-huggingface, langchain-pinecone, langchain\n",
      "Successfully installed SQLAlchemy-2.0.44 aiohappyeyeballs-2.6.1 aiohttp-3.13.1 aiohttp-retry-2.9.1 aiosignal-1.4.0 annotated-types-0.7.0 distro-1.9.0 frozenlist-1.8.0 greenlet-3.2.4 jiter-0.11.1 jsonpatch-1.33 langchain-0.3.27 langchain-core-0.3.79 langchain-huggingface-0.3.1 langchain-openai-0.3.35 langchain-pinecone-0.2.12 langchain-text-splitters-0.3.11 langsmith-0.4.37 multidict-6.7.0 openai-2.5.0 orjson-3.11.3 propcache-0.4.1 pydantic-2.12.3 pydantic-core-2.41.4 requests-toolbelt-1.0.0 simsimd-6.5.3 tenacity-9.1.2 tiktoken-0.12.0 typing-inspection-0.4.2 yarl-1.22.0 zstandard-0.25.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain-pinecone langchain-huggingface pinecone-client sentence-transformers dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba1566e",
   "metadata": {},
   "source": [
    "## 1. Setup Environment\n",
    "First, let's load our API keys and other configuration details from a `.env` file. This is a best practice for keeping sensitive information out of the notebook.\n",
    "\n",
    "Create a file named `.env` in your project's root directory and add your keys like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a314cafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API keys loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Check if keys are loaded\n",
    "if not PINECONE_API_KEY or not GOOGLE_API_KEY:\n",
    "    print(\"API keys not found. Please create a .env file and add your keys.\")\n",
    "else:\n",
    "    print(\"API keys loaded successfully.\")\n",
    "\n",
    "# This is the name of our index in Pinecone\n",
    "PINECONE_INDEX_NAME = \"ikarus\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86baf6cb",
   "metadata": {},
   "source": [
    "## 2. Load Cleaned Data\n",
    "We'll now load the `cleaned_data.csv` file that we prepared in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bfc7f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 312 records from cleaned_data.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniq_id</th>\n",
       "      <th>title</th>\n",
       "      <th>brand</th>\n",
       "      <th>price</th>\n",
       "      <th>images</th>\n",
       "      <th>categories</th>\n",
       "      <th>material</th>\n",
       "      <th>color</th>\n",
       "      <th>combined_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02593e81-5c09-5069-8516-b0b29f439ded</td>\n",
       "      <td>GOYMFK 1pc Free Standing Shoe Rack, Multi-laye...</td>\n",
       "      <td>GOYMFK</td>\n",
       "      <td>24.99</td>\n",
       "      <td>['https://m.media-amazon.com/images/I/416WaLx1...</td>\n",
       "      <td>['Home &amp; Kitchen', 'Storage &amp; Organization', '...</td>\n",
       "      <td>Metal</td>\n",
       "      <td>White</td>\n",
       "      <td>GOYMFK 1pc Free Standing Shoe Rack, Multi-laye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5938d217-b8c5-5d3e-b1cf-e28e340f292e</td>\n",
       "      <td>subrtex Leather ding Room, Dining Chairs Set o...</td>\n",
       "      <td>subrtex</td>\n",
       "      <td>53.99</td>\n",
       "      <td>['https://m.media-amazon.com/images/I/31SejUEW...</td>\n",
       "      <td>['Home &amp; Kitchen', 'Furniture', 'Dining Room F...</td>\n",
       "      <td>Sponge</td>\n",
       "      <td>Black</td>\n",
       "      <td>subrtex Leather ding Room, Dining Chairs Set o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b2ede786-3f51-5a45-9a5b-bcf856958cd8</td>\n",
       "      <td>Plant Repotting Mat MUYETOL Waterproof Transpl...</td>\n",
       "      <td>MUYETOL</td>\n",
       "      <td>5.98</td>\n",
       "      <td>['https://m.media-amazon.com/images/I/41RgefVq...</td>\n",
       "      <td>['Patio, Lawn &amp; Garden', 'Outdoor Décor', 'Doo...</td>\n",
       "      <td>Polyethylene</td>\n",
       "      <td>Green</td>\n",
       "      <td>Plant Repotting Mat MUYETOL Waterproof Transpl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8fd9377b-cfa6-5f10-835c-6b8eca2816b5</td>\n",
       "      <td>Pickleball Doormat, Welcome Doormat Absorbent ...</td>\n",
       "      <td>VEWETOL</td>\n",
       "      <td>13.99</td>\n",
       "      <td>['https://m.media-amazon.com/images/I/61vz1Igl...</td>\n",
       "      <td>['Patio, Lawn &amp; Garden', 'Outdoor Décor', 'Doo...</td>\n",
       "      <td>Rubber</td>\n",
       "      <td>A5589</td>\n",
       "      <td>Pickleball Doormat, Welcome Doormat Absorbent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bdc9aa30-9439-50dc-8e89-213ea211d66a</td>\n",
       "      <td>JOIN IRON Foldable TV Trays for Eating Set of ...</td>\n",
       "      <td>JOIN IRON Store</td>\n",
       "      <td>89.99</td>\n",
       "      <td>['https://m.media-amazon.com/images/I/41p4d4VJ...</td>\n",
       "      <td>['Home &amp; Kitchen', 'Furniture', 'Game &amp; Recrea...</td>\n",
       "      <td>Iron</td>\n",
       "      <td>Grey Set of 4</td>\n",
       "      <td>JOIN IRON Foldable TV Trays for Eating Set of ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                uniq_id  \\\n",
       "0  02593e81-5c09-5069-8516-b0b29f439ded   \n",
       "1  5938d217-b8c5-5d3e-b1cf-e28e340f292e   \n",
       "2  b2ede786-3f51-5a45-9a5b-bcf856958cd8   \n",
       "3  8fd9377b-cfa6-5f10-835c-6b8eca2816b5   \n",
       "4  bdc9aa30-9439-50dc-8e89-213ea211d66a   \n",
       "\n",
       "                                               title            brand  price  \\\n",
       "0  GOYMFK 1pc Free Standing Shoe Rack, Multi-laye...           GOYMFK  24.99   \n",
       "1  subrtex Leather ding Room, Dining Chairs Set o...          subrtex  53.99   \n",
       "2  Plant Repotting Mat MUYETOL Waterproof Transpl...          MUYETOL   5.98   \n",
       "3  Pickleball Doormat, Welcome Doormat Absorbent ...          VEWETOL  13.99   \n",
       "4  JOIN IRON Foldable TV Trays for Eating Set of ...  JOIN IRON Store  89.99   \n",
       "\n",
       "                                              images  \\\n",
       "0  ['https://m.media-amazon.com/images/I/416WaLx1...   \n",
       "1  ['https://m.media-amazon.com/images/I/31SejUEW...   \n",
       "2  ['https://m.media-amazon.com/images/I/41RgefVq...   \n",
       "3  ['https://m.media-amazon.com/images/I/61vz1Igl...   \n",
       "4  ['https://m.media-amazon.com/images/I/41p4d4VJ...   \n",
       "\n",
       "                                          categories      material  \\\n",
       "0  ['Home & Kitchen', 'Storage & Organization', '...         Metal   \n",
       "1  ['Home & Kitchen', 'Furniture', 'Dining Room F...        Sponge   \n",
       "2  ['Patio, Lawn & Garden', 'Outdoor Décor', 'Doo...  Polyethylene   \n",
       "3  ['Patio, Lawn & Garden', 'Outdoor Décor', 'Doo...        Rubber   \n",
       "4  ['Home & Kitchen', 'Furniture', 'Game & Recrea...          Iron   \n",
       "\n",
       "           color                                      combined_text  \n",
       "0          White  GOYMFK 1pc Free Standing Shoe Rack, Multi-laye...  \n",
       "1          Black  subrtex Leather ding Room, Dining Chairs Set o...  \n",
       "2          Green  Plant Repotting Mat MUYETOL Waterproof Transpl...  \n",
       "3          A5589  Pickleball Doormat, Welcome Doormat Absorbent ...  \n",
       "4  Grey Set of 4  JOIN IRON Foldable TV Trays for Eating Set of ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('cleaned_data.csv')\n",
    "\n",
    "# Handle potential empty rows\n",
    "df.dropna(subset=['uniq_id', 'combined_text'], inplace=True)\n",
    "\n",
    "# Display the first few rows\n",
    "print(f\"Loaded {len(df)} records from cleaned_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46da58e5",
   "metadata": {},
   "source": [
    "## 3. Initialize the Embedding Model with LangChain\n",
    "We will use a sentence-transformer model from HuggingFace to convert our `combined_text` field into dense vector embeddings. `all-MiniLM-L6-v2` is a great choice as it's efficient and effective for semantic search tasks. LangChain's `HuggingFaceEmbeddings` wrapper makes this incredibly simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67e327e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model initialized.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize the embedding model\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_kwargs = {\"device\": \"cpu\"} # Use \"cuda\" if you have a GPU\n",
    "encode_kwargs = {\"normalize_embeddings\": False}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "print(\"Embedding model initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2acdacd",
   "metadata": {},
   "source": [
    "## 4. Set Up Pinecone Vector Store\n",
    "Now, we'll connect to Pinecone. We will check if our desired index already exists. If not, we will create it.\n",
    "\n",
    "**Important:** The `dimension` of the index *must* match the output dimension of our embedding model. For `all-MiniLM-L6-v2`, this is **384**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d2f3b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arnav\\OneDrive\\Desktop\\Ikarus\\notebooks\\venv\\Lib\\site-packages\\langchain_pinecone\\__init__.py:3: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_pinecone.vectorstores import Pinecone, PineconeVectorStore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'ikarus' already exists.\n",
      "Pinecone vector store initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# Initialize Pinecone client\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# Check if the index already exists\n",
    "if PINECONE_INDEX_NAME not in pc.list_indexes().names():\n",
    "    print(f\"Creating index '{PINECONE_INDEX_NAME}'...\")\n",
    "    pc.create_index(\n",
    "        name=PINECONE_INDEX_NAME,\n",
    "        dimension=384,  # This MUST match your embedding model\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "    print(\"Index created successfully.\")\n",
    "else:\n",
    "    print(f\"Index '{PINECONE_INDEX_NAME}' already exists.\")\n",
    "\n",
    "# Get the index object\n",
    "index = pc.Index(PINECONE_INDEX_NAME)\n",
    "\n",
    "# Pass the index to LangChain\n",
    "vectorstore = PineconeVectorStore(index=index, embedding=embeddings)\n",
    "print(\"Pinecone vector store initialized successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0bb472",
   "metadata": {},
   "source": [
    "## 5. Generate and Upsert Embeddings\n",
    "This is the final and most important step. We will iterate through our DataFrame in batches, generate embeddings for the `combined_text` of each product, and then \"upsert\" (upload/insert) them into our Pinecone index.\n",
    "\n",
    "The `metadata` for each vector will include the product's title, brand, price, and image URLs, so we can retrieve this information during our search without needing another database lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b437d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8100e77865304b6393ae9587cc2fee2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upserting to Pinecone:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Embedding and Upserting Complete ---\n",
      "All 312 product records have been processed and stored in the 'ikarus' index.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm  # For progress bar\n",
    "\n",
    "# We'll process the data in batches to be efficient\n",
    "batch_size = 100\n",
    "\n",
    "for i in tqdm(range(0, len(df), batch_size), desc=\"Upserting to Pinecone\"):\n",
    "    # Get the batch of data\n",
    "    i_end = min(i + batch_size, len(df))\n",
    "    batch = df.iloc[i:i_end]\n",
    "\n",
    "    # Extract fields\n",
    "    ids = batch[\"uniq_id\"].astype(str).tolist()\n",
    "    texts = batch[\"combined_text\"].astype(str).tolist()\n",
    "\n",
    "    # Prepare metadata — ensure all fields are JSON serializable\n",
    "    metadata = [\n",
    "        {\n",
    "            \"title\": str(row.get(\"title\", \"\")),\n",
    "            \"brand\": str(row.get(\"brand\", \"\")),\n",
    "            \"price\": float(row.get(\"price\", 0.0)) if pd.notnull(row.get(\"price\")) else 0.0,\n",
    "            \"images\": str(row.get(\"images\", \"[]\")),\n",
    "        }\n",
    "        for _, row in batch.iterrows()\n",
    "    ]\n",
    "\n",
    "    # ✅ Add documents to Pinecone via LangChain vectorstore\n",
    "    # (LangChain handles embedding and upsert automatically)\n",
    "    vectorstore.add_texts(texts=texts, ids=ids, metadatas=metadata)\n",
    "\n",
    "print(\"\\n--- Embedding and Upserting Complete ---\")\n",
    "print(f\"All {len(df)} product records have been processed and stored in the '{PINECONE_INDEX_NAME}' index.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec062e9",
   "metadata": {},
   "source": [
    "### Verification (Optional)\n",
    "You can run a quick similarity search to verify that the data has been indexed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54837c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for query: 'A comfortable chair for a living room'\\n\n",
      "Title: Karl home Accent Chair Mid-Century Modern Chair with Pillow Upholstered Lounge Arm Chair with Solid Wood Frame & Soft Cushion for Living Room, Bedroom, Belcony, Beige\n",
      "Brand: Karl home Store\n",
      "Price: $149.99\n",
      "------------------------------\n",
      "Title: Ergonomic Office Chair,Office Chair, with Lumbar Support & 3D Headrest & Flip Up Arms Home Office Desk Chairs Rockable High Back Swivel Computer Chair White Frame Mesh Study Chair（All Black）\n",
      "Brand: SCaua\n",
      "Price: $126.99\n",
      "------------------------------\n",
      "Title: Lazy Chair with Ottoman, Modern Lounge Accent Chair with Footrest, Pillow and Blanket, Leisure Sofa Chair Reading Chair with Armrests and Side Pocket for Living Room, Bedroom & Small Space, Grey\n",
      "Brand: WARMGIFT WM\n",
      "Price: $139.99\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run a quick test query\n",
    "query = \"A comfortable chair for a living room\"\n",
    "\n",
    "try:\n",
    "    results = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "    print(f\"Results for query: '{query}'\\\\n\")\n",
    "    for doc in results:\n",
    "        print(f\"Title: {doc.metadata.get('title')}\")\n",
    "        print(f\"Brand: {doc.metadata.get('brand')}\")\n",
    "        print(f\"Price: ${doc.metadata.get('price')}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during the test query: {e}\")\n",
    "    print(\"This might happen if the index is still initializing. Please wait a few minutes and try again.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
